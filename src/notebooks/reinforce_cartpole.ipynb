{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "file_extension": ".py",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": 3,
    "colab": {
      "name": "reinforce_cartpole.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdhJQiL6E6m7",
        "colab_type": "text"
      },
      "source": [
        "# REINFORCE on CartPole"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HydYfHuHCLr1",
        "colab_type": "text"
      },
      "source": [
        "This notebook applies our REINFORCE in a Cartpole setting, to verify correctness of our implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2I66qbFHE6m8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLVKuSAHE6nA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make('CartPole-v1')\n",
        "env.seed(1)\n",
        "torch.manual_seed(1)\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.01\n",
        "gamma = 0.99"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VD-Suew6E6nD",
        "colab_type": "text"
      },
      "source": [
        "# Define model and training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNf5Ka4ZE6nE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Policy(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Policy, self).__init__()\n",
        "        state_space = env.observation_space.shape[0]\n",
        "        action_space = env.action_space.n\n",
        "        num_hidden = 128\n",
        "\n",
        "        self.l1 = nn.Linear(state_space, num_hidden, bias=False)\n",
        "        self.l2 = nn.Linear(num_hidden, action_space, bias=False)\n",
        "\n",
        "        # Overall reward and loss history\n",
        "        self.reward_history = []\n",
        "        self.loss_history = []\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        # Episode policy and reward history\n",
        "        self.saved_log_probs = []\n",
        "        self.rewards = []\n",
        "\n",
        "    def forward(self, x):\n",
        "        model = torch.nn.Sequential(\n",
        "            self.l1,\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.ReLU(),\n",
        "            self.l2,\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "        return model(x)\n",
        "\n",
        "\n",
        "class Value(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Value, self).__init__()\n",
        "        state_space = env.observation_space.shape[0]\n",
        "        num_hidden = 128\n",
        "\n",
        "        self.l1 = nn.Linear(state_space, num_hidden, bias=False)\n",
        "        self.l2 = nn.Linear(num_hidden, 1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        model = torch.nn.Sequential(\n",
        "            self.l1,\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.ReLU(),\n",
        "            self.l2,\n",
        "        )\n",
        "        return model(x)\n",
        "\n",
        "\n",
        "def predict(state):\n",
        "    # Select an action (0 or 1) by running policy model\n",
        "    # and choosing based on the probabilities in state\n",
        "    state = torch.from_numpy(state).type(torch.FloatTensor).to(device)\n",
        "    action_probs = policy(state)\n",
        "    distribution = Categorical(action_probs)\n",
        "    action = distribution.sample()\n",
        "\n",
        "    # Add log probability of our chosen action to our history\n",
        "    policy.saved_log_probs.append(distribution.log_prob(action))\n",
        "    \n",
        "    return action\n",
        "\n",
        "\n",
        "def update_policy(replay):\n",
        "    R = 0\n",
        "    policy_loss = []\n",
        "    returns = []\n",
        "    for r in policy.rewards[::-1]:\n",
        "        R = r + gamma * R\n",
        "        returns.insert(0, R)\n",
        "    returns = torch.tensor(returns).float().to(device)\n",
        "    states = torch.tensor([state for state, _, _ in replay], dtype=torch.float, device=device)\n",
        "    vals = value(states).squeeze(1) \n",
        "    if not math.isnan(returns.std()):\n",
        "        returns = (returns - returns.mean()) / \\\n",
        "            (returns.std() + np.finfo(np.float32).eps)\n",
        "    with torch.no_grad():\n",
        "        advantage = returns - vals\n",
        "\n",
        "    for log_prob, R in zip(policy.saved_log_probs, advantage):\n",
        "        policy_loss.append(-log_prob * R)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    policy_loss = torch.stack(policy_loss).sum().to(device)\n",
        "    policy_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    val_optimizer.zero_grad()\n",
        "    F.mse_loss(vals, returns).backward()\n",
        "    val_optimizer.step()\n",
        "\n",
        "    # Save and intialize episode history counters\n",
        "    policy.loss_history.append(policy_loss.item())\n",
        "    policy.reward_history.append(np.sum(policy.rewards))\n",
        "    policy.reset()\n",
        "    del policy.rewards[:]\n",
        "    del policy.saved_log_probs[:]\n",
        "\n",
        "\n",
        "def train(episodes):\n",
        "    scores = []\n",
        "    replay = []\n",
        "    for episode in range(episodes):\n",
        "        # Reset environment and record the starting state\n",
        "        state = env.reset()\n",
        "\n",
        "        for time in range(1000):\n",
        "            action = predict(state)\n",
        "\n",
        "            # Uncomment to render the visual state in a window\n",
        "            # env.render()\n",
        "\n",
        "            # Step through environment using chosen action\n",
        "            next_state, reward, done, _ = env.step(action.item())\n",
        "            replay.append((state, action, reward))\n",
        "            state = next_state\n",
        "\n",
        "            # Save reward\n",
        "            policy.rewards.append(reward)\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        update_policy(replay)\n",
        "        replay = []\n",
        "        # Calculate score to determine when the environment has been solved\n",
        "        scores.append(time)\n",
        "        mean_score = np.mean(scores[-100:])\n",
        "\n",
        "        if episode % 50 == 0:\n",
        "            print('Episode {}\\tAverage length (last 100 episodes): {:.2f}'.format(\n",
        "                episode, mean_score))\n",
        "\n",
        "        if mean_score > env.spec.reward_threshold:\n",
        "            print(\"Solved after {} episodes! Running average is now {}. Last episode ran to {} time steps.\"\n",
        "                  .format(episode, mean_score, time))\n",
        "            break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZm44k4tE6nH",
        "colab_type": "text"
      },
      "source": [
        "# Start training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_yGc-rbE6nH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(device)\n",
        "policy = Policy().to(device)\n",
        "value = Value().to(device)\n",
        "optimizer = optim.Adam(policy.parameters(), lr=learning_rate)\n",
        "val_optimizer = optim.Adam(value.parameters(), lr=learning_rate)\n",
        "train(episodes=1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04-GEplqE6nL",
        "colab_type": "text"
      },
      "source": [
        "# Plot training performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHMx3KdjE6nL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# number of episodes for rolling average\n",
        "window = 50\n",
        "\n",
        "fig, ((ax1), (ax2)) = plt.subplots(2, 1, sharey=True, figsize=[9, 9])\n",
        "rolling_mean = pd.Series(policy.reward_history).rolling(window).mean()\n",
        "std = pd.Series(policy.reward_history).rolling(window).std()\n",
        "ax1.plot(rolling_mean)\n",
        "ax1.fill_between(range(len(policy.reward_history)), rolling_mean -\n",
        "                 std, rolling_mean+std, color='orange', alpha=0.2)\n",
        "ax1.set_title(\n",
        "    'Episode Length Moving Average ({}-episode window)'.format(window))\n",
        "ax1.set_xlabel('Episode')\n",
        "ax1.set_ylabel('Episode Length')\n",
        "\n",
        "ax2.plot(policy.reward_history)\n",
        "ax2.set_title('Episode Length')\n",
        "ax2.set_xlabel('Episode')\n",
        "ax2.set_ylabel('Episode Length')\n",
        "\n",
        "fig.tight_layout(pad=2)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}