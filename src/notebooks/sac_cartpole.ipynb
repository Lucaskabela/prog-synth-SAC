{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "file_extension": ".py",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": 3,
    "colab": {
      "name": "sac_cartpole.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdhJQiL6E6m7",
        "colab_type": "text"
      },
      "source": [
        "# SAC on CartPole"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgS9ufbdDQKG",
        "colab_type": "text"
      },
      "source": [
        "This notebook applies our SAC in a Cartpole setting, to verify correctness of our implementation.  Note, we use the Gumbel Softmax trick here for discrete action space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2I66qbFHE6m8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import pandas as pd\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from collections import namedtuple, deque\n",
        "from torch.distributions import Categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLVKuSAHE6nA",
        "colab_type": "code",
        "outputId": "62cd5757-f9b0-4629-f1b9-13e3c2202446",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "env = gym.make('CartPole-v1')\n",
        "print(torch.__version__)\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 1e-4\n",
        "gamma = 0.99\n",
        "soft_tau=5e-3\n",
        "batch_size = 256"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.5.0+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VD-Suew6E6nD",
        "colab_type": "text"
      },
      "source": [
        "# Define model and training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZAV43R3cgEm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class Replay_Buffer(object):\n",
        "    \"\"\"Replay buffer to store past experiences that the agent can then use for training data\"\"\"\n",
        "    \n",
        "    def __init__(self, buffer_size, batch_size, seed=0):\n",
        "\n",
        "        self.memory = deque(maxlen=buffer_size)\n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "        self.seed = random.seed(seed)\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    def add_experience(self, states, actions, rewards, next_states, dones):\n",
        "        \"\"\"Adds experience(s) into the replay buffer\"\"\"\n",
        "        if type(dones) == list:\n",
        "            assert type(dones[0]) != list, \"A done shouldn't be a list\"\n",
        "            experiences = [self.experience(state, action, reward, next_state, done)\n",
        "                           for state, action, reward, next_state, done in\n",
        "                           zip(states, actions, rewards, next_states, dones)]\n",
        "            self.memory.extend(experiences)\n",
        "        else:\n",
        "            experience = self.experience(states, actions, rewards, next_states, dones)\n",
        "            self.memory.append(experience)\n",
        "   \n",
        "    def sample(self, num_experiences=None, separate_out_data_types=True):\n",
        "        \"\"\"Draws a random sample of experience from the replay buffer\"\"\"\n",
        "        experiences = self.pick_experiences(num_experiences)\n",
        "        if separate_out_data_types:\n",
        "            states, actions, rewards, next_states, dones = self.separate_out_data_types(experiences)\n",
        "            return states, actions, rewards, next_states, dones\n",
        "        else:\n",
        "            return experiences\n",
        "            \n",
        "    def separate_out_data_types(self, experiences):\n",
        "        \"\"\"Puts the sampled experience into the correct format for a PyTorch neural network\"\"\"\n",
        "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(self.device)\n",
        "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(self.device)\n",
        "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(self.device)\n",
        "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(self.device)\n",
        "        dones = torch.from_numpy(np.vstack([int(e.done) for e in experiences if e is not None])).float().to(self.device)\n",
        "        \n",
        "        return states, actions, rewards, next_states, dones\n",
        "    \n",
        "    def pick_experiences(self, num_experiences=None):\n",
        "        if num_experiences is not None: batch_size = num_experiences\n",
        "        else: batch_size = self.batch_size\n",
        "        return random.sample(self.memory, k=batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vyr6UzxgpPIS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://stackoverflow.com/questions/56226133/soft-actor-critic-with-discrete-action-space\n",
        "# ... for discrete action, GumbelSoftmax distribution\n",
        "class GumbelSoftmax(torch.distributions.RelaxedOneHotCategorical):\n",
        "    '''\n",
        "    A differentiable Categorical distribution using reparametrization trick with Gumbel-Softmax\n",
        "    Explanation http://amid.fish/assets/gumbel.html\n",
        "    NOTE: use this in place PyTorch's RelaxedOneHotCategorical distribution since its log_prob is not working right (returns positive values)\n",
        "    Papers:\n",
        "    [1] The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables (Maddison et al, 2017)\n",
        "    [2] Categorical Reparametrization with Gumbel-Softmax (Jang et al, 2017)\n",
        "    '''\n",
        "\n",
        "    def sample(self, sample_shape=torch.Size()):\n",
        "        '''Gumbel-softmax sampling. Note rsample is inherited from RelaxedOneHotCategorical'''\n",
        "        u = torch.empty(self.logits.size(), device=self.logits.device, dtype=self.logits.dtype).uniform_(0, 1)\n",
        "        noisy_logits = self.logits - torch.log(-torch.log(u))\n",
        "        return torch.argmax(noisy_logits, dim=-1)\n",
        "\n",
        "    def log_prob(self, value):\n",
        "        '''value is one-hot or relaxed'''\n",
        "        if value.shape != self.logits.shape:\n",
        "            value = F.one_hot(value.long(), self.logits.shape[-1]).float()\n",
        "            assert value.shape == self.logits.shape\n",
        "        return - torch.sum(- value * F.log_softmax(self.logits, -1), -1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqM48NnVchAM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Policy(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Policy, self).__init__()\n",
        "        state_space = env.observation_space.shape[0]\n",
        "        action_space = env.action_space.n\n",
        "        num_hidden = 256\n",
        "        target_entropy_ratio = .95\n",
        "\n",
        "        self.l1 = nn.Linear(state_space, num_hidden)\n",
        "        self.l2 = nn.Linear(num_hidden, num_hidden)\n",
        "        self.l3 = nn.Linear(num_hidden, action_space)\n",
        "\n",
        "        self.target_entropy = -np.log(1.0/env.action_space.n) * target_entropy_ratio\n",
        "        self.log_alpha = torch.zeros(1, requires_grad=True, device=device)\n",
        "        self.alpha = self.log_alpha.detach().exp()\n",
        "\n",
        "        # Overall reward and loss history\n",
        "        self.reward_history = []\n",
        "        self.loss_history = []\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        # Episode policy and reward history\n",
        "        self.saved_log_probs = []\n",
        "        self.rewards = []\n",
        "\n",
        "    def forward(self, x):\n",
        "        model = torch.nn.Sequential(\n",
        "            self.l1,\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.ReLU(),\n",
        "            self.l2,\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.ReLU(),\n",
        "            self.l3,\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "        return model(x)\n",
        "\n",
        "    def act(self, state):\n",
        "        # Select an action (0 or 1) by running policy model\n",
        "        # and choosing based on the probabilities in state\n",
        "        action_probs = self.forward(state)\n",
        "        action = torch.distributions.Categorical(probs=action_probs).sample().cpu()\n",
        "        return action.cpu().squeeze()\n",
        "\n",
        "    def calc_log_prob_action(self, state, reparam=False):\n",
        "        \n",
        "        action_probs = self.forward(state)\n",
        "        action_pd = GumbelSoftmax(probs=action_probs, temperature=.9)\n",
        "        actions = action_pd.rsample() if reparam else action_pd.sample()\n",
        "        log_probs = action_pd.log_prob(actions)\n",
        "        return log_probs, actions\n",
        "\n",
        "\n",
        "class SoftQNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SoftQNetwork, self).__init__()\n",
        "        self.state_space = env.observation_space.shape[0]\n",
        "        self.action_space = env.action_space.n\n",
        "        num_hidden = 256\n",
        "\n",
        "        self.l1 = nn.Linear(self.state_space + self.action_space, num_hidden)\n",
        "        self.l2 = nn.Linear(num_hidden, num_hidden)\n",
        "        self.l3 = nn.Linear(num_hidden, 1)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        x = torch.cat([x, y], 1)\n",
        "        model = torch.nn.Sequential(\n",
        "            self.l1,\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.ReLU(),\n",
        "            self.l2,\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.ReLU(),\n",
        "            self.l3\n",
        "        )\n",
        "        return model(x).view(-1)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "468fNumAcj_J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def guard_q_actions(actions, dim):\n",
        "    '''Guard to convert actions to one-hot for input to Q-network'''\n",
        "    actions = F.one_hot(actions.long(), dim).float()\n",
        "    return actions\n",
        "\n",
        "def calculate_critic_loss(states, actions, rewards, next_states, done):\n",
        "    with torch.no_grad():\n",
        "        next_probs, next_actions = policy.calc_log_prob_action(next_states)\n",
        "        next_actions = guard_q_actions(next_actions, tgt_q_net1.action_space)\n",
        "        next_q1 = tgt_q_net1(next_states, next_actions)\n",
        "        next_q2 = tgt_q_net2(next_states, next_actions)\n",
        "\n",
        "        min_q_next = (torch.min(next_q1, next_q2) - policy.alpha * next_probs)\n",
        "        target_q_value = rewards + (1 - done) * gamma * min_q_next\n",
        "\n",
        "    p_q1 = soft_q_net1(states, actions)\n",
        "    p_q2 = soft_q_net2(states, actions)\n",
        "    q_value_loss1 = F.mse_loss(p_q1, target_q_value)\n",
        "    q_value_loss2 = F.mse_loss(p_q2, target_q_value)\n",
        "    return q_value_loss1, q_value_loss2\n",
        "\n",
        "def calculate_actor_loss(states):\n",
        "     # Train actor network\n",
        "    log_probs, actions = policy.calc_log_prob_action(states, reparam=True)\n",
        "    q1 = soft_q_net1(states, actions)\n",
        "    q2 = soft_q_net2(states, actions)\n",
        "    min_q = torch.min(q1, q2)\n",
        "    policy_loss = (policy.alpha * log_probs - min_q).mean()\n",
        "    return policy_loss, log_probs\n",
        "\n",
        "def calculate_entropy_tuning_loss(log_pi):\n",
        "    \"\"\"Calculates the loss for the entropy temperature parameter. This is only relevant if self.automatic_entropy_tuning\n",
        "    is True.\"\"\"\n",
        "    alpha_loss = -(policy.log_alpha * (log_pi.detach() + policy.target_entropy)).mean()\n",
        "    return alpha_loss\n",
        "\n",
        "def update(replay):\n",
        "    # Soft Q networks are the problem, detach gradient?  Not sure why they are the cause\n",
        "    states, action, reward, next_states, done = replay.sample(batch_size)\n",
        "    action = action.squeeze(1)\n",
        "    reward = reward.squeeze(1)\n",
        "    done = done.squeeze(1)\n",
        "    if not math.isnan(reward.std()):\n",
        "        reward = (reward - reward.mean()) / (reward.std() + np.finfo(np.float32).eps)\n",
        "    action = guard_q_actions(action, soft_q_net1.action_space)\n",
        "    q_value_loss1, q_value_loss2 = calculate_critic_loss(states, action, reward, next_states, done) \n",
        "    q1_opt.zero_grad()\n",
        "    q_value_loss1.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(soft_q_net1.parameters(), 5.)\n",
        "    q1_opt.step()\n",
        "    q2_opt.zero_grad()\n",
        "    q_value_loss2.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(soft_q_net2.parameters(), 5.)\n",
        "    q2_opt.step()  \n",
        "\n",
        "    policy_loss, log_action_probabilities = calculate_actor_loss(states)\n",
        "    policy_opt.zero_grad()\n",
        "    policy_loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(policy.parameters(), 5.)\n",
        "    policy_opt.step()\n",
        "\n",
        "    alpha_loss = calculate_entropy_tuning_loss(log_action_probabilities) \n",
        "    entropy_opt.zero_grad()\n",
        "    alpha_loss.backward()\n",
        "    entropy_opt.step()\n",
        "    \n",
        "    loss = q_value_loss1 + q_value_loss2 + policy_loss + alpha_loss\n",
        "    policy.alpha = policy.log_alpha.detach().exp()\n",
        "\n",
        "    for target_param, param in zip(tgt_q_net1.parameters(), soft_q_net1.parameters()):\n",
        "        target_param.data.copy_(\n",
        "            target_param.data * (1.0 - soft_tau) + param.data * soft_tau\n",
        "        )\n",
        "    \n",
        "    for target_param, param in zip(tgt_q_net2.parameters(), soft_q_net2.parameters()):\n",
        "        target_param.data.copy_(\n",
        "            target_param.data * (1.0 - soft_tau) + param.data * soft_tau\n",
        "        )\n",
        "    # Save and intialize episode history counters\n",
        "    policy.loss_history.append(policy_loss.item())\n",
        "    policy.reset()\n",
        "    del policy.rewards[:]\n",
        "    del policy.saved_log_probs[:]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNf5Ka4ZE6nE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def train(episodes):\n",
        "    scores = []\n",
        "    replay = Replay_Buffer(1000000, batch_size)\n",
        "    global_step = 0\n",
        "    for episode in range(episodes):\n",
        "        # Reset environment and record the starting state\n",
        "        state = env.reset()\n",
        "        reward_cum = 0\n",
        "        for time in range(1000):\n",
        "            state_inp = torch.from_numpy(state).float().to(device)\n",
        "            if (global_step < 100):\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                action = policy.act(state_inp)\n",
        "                action = action.item()\n",
        "\n",
        "            # Uncomment to render the visual state in a window\n",
        "            # Step through environment using chosen action\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            replay.add_experience(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            reward_cum += reward\n",
        "            global_step +=1\n",
        "            if len(replay) > batch_size:\n",
        "                update(replay)\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # Calculate score to determine when the environment has been solved\n",
        "        scores.append(time)\n",
        "        policy.reward_history.append(reward_cum)\n",
        "        mean_score = np.mean(scores[-100:])\n",
        "\n",
        "        if episode % 50 == 0:\n",
        "            print('Episode {}\\tAverage length (last 100 episodes): {:.2f}'.format(\n",
        "                episode, mean_score))\n",
        "\n",
        "        if mean_score > env.spec.reward_threshold:\n",
        "            print(\"Solved after {} episodes! Running average is now {}. Last episode ran to {} time steps.\"\n",
        "                  .format(episode, mean_score, time))\n",
        "            break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZm44k4tE6nH",
        "colab_type": "text"
      },
      "source": [
        "# Start training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_yGc-rbE6nH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(device)\n",
        "policy = Policy().to(device)\n",
        "soft_q_net1 = SoftQNetwork().to(device)\n",
        "soft_q_net2 = SoftQNetwork().to(device)\n",
        "tgt_q_net1 = SoftQNetwork().to(device).eval()\n",
        "tgt_q_net2 = SoftQNetwork().to(device).eval()\n",
        "\n",
        "for target_param, param in zip(tgt_q_net1.parameters(), soft_q_net1.parameters()):\n",
        "    target_param.data.copy_( param.data)\n",
        "for target_param, param in zip(tgt_q_net2.parameters(), soft_q_net2.parameters()):\n",
        "    target_param.data.copy_( param.data)\n",
        "for param in tgt_q_net1.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in tgt_q_net2.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "policy_opt = optim.Adam(policy.parameters(), lr=learning_rate, eps=1e-4)\n",
        "entropy_opt = optim.Adam([policy.log_alpha], lr=learning_rate, eps=1e-4)\n",
        "q1_opt = optim.Adam(soft_q_net1.parameters(), lr=learning_rate, eps=1e-4)\n",
        "q2_opt = optim.Adam(soft_q_net2.parameters(), lr=learning_rate, eps=1e-4)\n",
        "train(episodes=1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04-GEplqE6nL",
        "colab_type": "text"
      },
      "source": [
        "# Plot training performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHMx3KdjE6nL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# number of episodes for rolling average\n",
        "window = 50\n",
        "\n",
        "fig, ((ax1), (ax2)) = plt.subplots(2, 1, sharey=True, figsize=[9, 9])\n",
        "rolling_mean = pd.Series(policy.reward_history).rolling(window).mean()\n",
        "std = pd.Series(policy.reward_history).rolling(window).std()\n",
        "ax1.plot(rolling_mean)\n",
        "ax1.fill_between(range(len(policy.reward_history)), rolling_mean -\n",
        "                 std, rolling_mean+std, color='orange', alpha=0.2)\n",
        "ax1.set_title(\n",
        "    'Episode Length Moving Average ({}-episode window)'.format(window))\n",
        "ax1.set_xlabel('Episode')\n",
        "ax1.set_ylabel('Episode Length')\n",
        "\n",
        "ax2.plot(policy.reward_history)\n",
        "ax2.set_title('Episode Length')\n",
        "ax2.set_xlabel('Episode')\n",
        "ax2.set_ylabel('Episode Length')\n",
        "\n",
        "fig.tight_layout(pad=2)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}